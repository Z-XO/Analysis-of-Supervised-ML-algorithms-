{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries;\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set_style('white')\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import genfromtxt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "from sklearn import tree\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireWall = pd.read_csv('log2.csv')\n",
    "\n",
    "fireWallXData = fireWall[['Source Port', 'Destination Port','NAT Source Port',\n",
    "                         'NAT Destination Port','Bytes','Bytes Sent','Bytes Received','Packets',\n",
    "                        'Elapsed Time (sec)','pkts_sent','pkts_received']]\n",
    "fireWallyData = fireWall[['Action']]\n",
    "\n",
    "X = fireWallXData.iloc[:,0:10]\n",
    "y = fireWallyData.iloc[:,0]\n",
    "\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "#print(fireWall.shape)\n",
    "\n",
    "FireWALLData = pd.concat([X, y], axis=1)\n",
    "print(FireWALLData.shape)\n",
    "\n",
    "newX = FireWALLData.iloc[:,0:9]\n",
    "newy = FireWALLData.iloc[:,10]\n",
    "FireWALLData.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encodeLabel = LabelEncoder()\n",
    "newy = encodeLabel.fit_transform(newy)\n",
    "print(newy)\n",
    "X_train, X_test, y_train, y_test = train_test_split( newX, newy, test_size=60000, random_state=1738)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FORREST TRIAL ONE ON Fire Wall DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 25000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', RandomForestClassifier(n_jobs = -1))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['auto'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': [ 'sqrt'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['log2'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']\n",
    "                \n",
    "                }\n",
    "                    ]\n",
    "                \n",
    "                \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_1 = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL ONE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_1.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_1.cv_results_['params'][ np.argmin(TRIAL_1.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_1.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_1.cv_results_['params'][ np.argmin(TRIAL_1.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_1.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_1.cv_results_['params'][ np.argmin(TRIAL_1.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_1.cv_results_['params'][ np.argmin(TRIAL_1.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 RF using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_1.cv_results_['params'][ np.argmin(TRIAL_1.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 RF using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_1.cv_results_['params'][ np.argmin(TRIAL_1.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 RF using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST TRIAL TWO ON Fire Wall DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 25000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', RandomForestClassifier(n_jobs = -1))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['auto'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': [ 'sqrt'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['log2'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']\n",
    "                \n",
    "                }\n",
    "                    ]\n",
    "                \n",
    "                \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_2 = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL TWO RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_2.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_2.cv_results_['params'][ np.argmin(TRIAL_2.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_2.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_2.cv_results_['params'][ np.argmin(TRIAL_2.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_2.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_2.cv_results_['params'][ np.argmin(TRIAL_2.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_2.cv_results_['params'][ np.argmin(TRIAL_2.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 RF using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_2.cv_results_['params'][ np.argmin(TRIAL_2.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 RF using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_2.cv_results_['params'][ np.argmin(TRIAL_2.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 RF using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST TRIAL THREE ON Fire Wall  DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 25000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', RandomForestClassifier(n_jobs = -1))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['auto'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': [ 'sqrt'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['log2'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']\n",
    "                \n",
    "                }\n",
    "                    ]\n",
    "                \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_3 = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL THREE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_3.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_3.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_3.cv_results_['params'][ np.argmin(TRIAL_3.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_3.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_3.cv_results_['params'][ np.argmin(TRIAL_3.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_3.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_3.cv_results_['params'][ np.argmin(TRIAL_3.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_3.cv_results_['params'][ np.argmin(TRIAL_3.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 RF using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_3.cv_results_['params'][ np.argmin(TRIAL_3.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 RF using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_3.cv_results_['params'][ np.argmin(TRIAL_3.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 RF using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST TRIAL FOUR ON Fire Wall DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 25000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', RandomForestClassifier(n_jobs = -1))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['auto'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': [ 'sqrt'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['log2'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']\n",
    "                \n",
    "                }\n",
    "                    ]\n",
    "                \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_4 = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_4.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_4.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_4.cv_results_['params'][ np.argmin(TRIAL_4.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_4.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_4.cv_results_['params'][ np.argmin(TRIAL_4.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_4.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_4.cv_results_['params'][ np.argmin(TRIAL_4.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_4.cv_results_['params'][ np.argmin(TRIAL_4.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 RF using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_4.cv_results_['params'][ np.argmin(TRIAL_4.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 RF using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_4.cv_results_['params'][ np.argmin(TRIAL_4.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 RF using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST TRIAL FIVE ON Fire Wall DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 25000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', RandomForestClassifier(n_jobs = -1))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['auto'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': [ 'sqrt'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']},\n",
    "                \n",
    "                {\n",
    "                \n",
    "                'classifier': [RandomForestClassifier()],\n",
    "                'classifier__n_estimators': [100,200,400],\n",
    "                'classifier__max_features': ['log2'],\n",
    "                'classifier__max_depth' : [7,9,11,13],\n",
    "                'classifier__criterion' :['gini', 'entropy']\n",
    "                \n",
    "                }\n",
    "                    ]\n",
    "                \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_5 = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_5.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_5.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_5.cv_results_['params'][ np.argmin(TRIAL_5.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_5.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_5.cv_results_['params'][ np.argmin(TRIAL_5.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_5.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_5.cv_results_['params'][ np.argmin(TRIAL_5.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_5.cv_results_['params'][ np.argmin(TRIAL_5.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 RF using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_5.cv_results_['params'][ np.argmin(TRIAL_4.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 RF using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_5.cv_results_['params'][ np.argmin(TRIAL_5.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 RF using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
